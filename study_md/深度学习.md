# 深度学习                                                           



标签（空格分隔）： 机器学习

[TOC]

---

## NMS(非极大值抑制)

针对候选框进行处理

第一步：将小于一定概率的box都舍去

第二步：选取概率最大的候选框，然后将与之重叠的舍去x


## 神经网络层

### 卷积层

卷积神经网络(Convolutional Neural Networks(CNN)),卷积神经网络是一种多层神经网络，擅长处理图像特别是大图像的相关机器学习问题。卷积层的作用就是将特征进行映射，卷积是对各个输入特征量进行乘积求和的过程，一定程度上保留了输入图像的特征。 其作用是对输入图片的特征进行缩放，降低网络参数的量级。


![image.png-15.6kB](http://static.zybuluo.com/ShowArcher/1tqkxkmac56k9e6swsclpvh5/image.png)



### 池化层

池化层(Pooling)分为最大值下采样（Max-Pooling）与平均值下采样（Mean-Pooling）,及通过池化核对输入图片求其最大或平均，这样做减少了许多数据，特征的统计属性仍能够描述图像，而且由于降低了数据维度，有效地避免了过拟合。

下采样 (MaxPooling) 

### 全连接层

全连接网络层卷积取的是局部特征，全连接就是把以前的局部特征重新通过权值矩阵组装成完整的图。
![image.png-147.2kB](http://static.zybuluo.com/ShowArcher/dgknay71w0wn3prtg90ungj8/image.png)

从上图我们可以看出，我们用一个3x3x5的filter 去卷积激活函数的输出，得到的结果就是一个fully connected layer 的一个神经元的输出，这个输出就是一个值。因为我们有4096个神经元。我们实际就是用一个3x3x5x4096的卷积层去卷积激活函数的输出。

### 批规范化

批规范化（Batch Normalization）层，也可以理解为归一化，比如对一个网络进行输入过程中，输入的参数有1，100，0.01，而经过一个网络层进行扩大或者缩小后，其输出值将产生巨大的差距

首先来说归一化的问题，神经网络训练开始前，都要对数据做一个归一化处理，归一化有很多好处，原因是网络学习的过程的本质就是学习数据分布，一旦训练数据和测试数据的分布不同，那么网络的泛化能力就会大大降低，另外一方面，每一批次的数据分布如果不相同的话，那么网络就要在每次迭代的时候都去适应不同的分布，这样会大大降低网络的训练速度，这也就是为什么要对数据做一个归一化预处理的原因。另外对图片进行归一化处理还可以处理光照，对比度等影响。
网络一旦训练起来，参数就要发生更新，出了输入层的数据外，其它层的数据分布是一直发生变化的，因为在训练的时候，网络参数的变化就会导致后面输入数据的分布变化，比如第二层输入，是由输入数据和第一层参数得到的，而第一层的参数随着训练一直变化，势必会引起第二层输入分布的改变，把这种改变称之为：Internal Covariate Shift，BN就是为了解决这个问题的。

### 优化器

> SGD

SGD是随机梯度下降（stochastic gradient descent), 关于优化器的Momentum,表示动量值,其对应的下降公式如： $v_t = v_t*momentum + g_t$ 所以在引入momentum后，可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。就像惯性一样，一般设置为0.9,默认为0.

    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) 


##  二分分类

以识别猫为例子：输入一幅图片，识别其是否为猫。

### 符号定义

![image.png-101kB](http://static.zybuluo.com/ShowArcher/j9benta1pzvzvk0hlk2wifim/image.png)

输入为$n_x*m$的矩阵 输出标签y为$1*m$的矩阵。将数据矩阵化的原因，是因为对矩阵做运算比利用迭代更新算法快很多。

> 激活函数sigmod

![image.png-127.7kB](http://static.zybuluo.com/ShowArcher/48i12mztuk17oectw0t8ww7r/image.png)

$$ \sigma(z) = \dfrac{1}{1+e^{-z}}$$

其激活函数sigmod图像如上所示，在输入值较小时，返回趋于0的值，而输入值较大时，返回趋于1的值。这样做相当于做了一个归一化，让输出值归一化到0~1区间，可以直观的反应其识别成功的概率。

> 损失函数和成本函数
  为了描述预测值$\hat{y}$和$y$之间的误差,定义了损失函数$loss function$进行描述:
  
$$ L({\hat{y},y}) = \dfrac{1}{2} * (\hat{y} - y)^2$$
$$ L({\hat{y},y}) = -[ylog\hat{y} + (1 - y)log(1 - \hat{y})]$$

对于以上两个损失函数，我们选择第二种.因为在梯度下降的过程中，第二种是凸函数优化，而第一种是非凸的问题。非凸即存在多个局部最优解。

> 为什么采取第二种损失函数
根据信息论，在给定x的条件下预测为$\hat{y}$和$y$的条件概率统一后如下:
$$y = 1\quad P(\hat{y}|x) = \hat{y}$$
$$y = 0\quad P(y|x) = 1 - \hat{y}$$
$$\Longrightarrow P(y|x) = \hat{y}^y(1 - \hat{y})^{(1-y)}$$

然后在取对数后，再添加负号，添加负号后为了最小化损失函数
$$ L({\hat{y},y}) = -[ylog\hat{y} + (1 - y)log(1 - \hat{y})]$$


接下来定义描述这个训练过程的误差函数，也叫成本函数(Cost function):

$$J(w, b) = \dfrac{1}{m}\sum_{i=1}^{m}L(\hat{y}, y)$$

### 梯度下降

梯度下降的过程就是对参数w不断求偏导更新的过程：
![image.png-117.3kB](http://static.zybuluo.com/ShowArcher/7hgickmjn16r7a1v5xcgzmbb/image.png)


> 关于反向传播：

![image.png-139.5kB](http://static.zybuluo.com/ShowArcher/q319955uuk7s2291rqgsy03p/image.png)

如上图所示，求偏导是从输出到输入的过程，即根据链式法则反向传播计算处梯度。所以梯度下降就是利用反向传播求偏导再计算出参数的过程。

> 对于n个样本的梯度下降算法

![image.png-128.1kB](http://static.zybuluo.com/ShowArcher/rz1eko5nifychwypk3rj30mz/image.png)


如上图所示，即对偏导数、损失函数、偏重求平均后再进行梯度下降

> 向量化

![image.png-285.9kB](http://static.zybuluo.com/ShowArcher/m2gcnnszput7o2z63yamklqa/image.png)

因为矩阵计算速度较快，所以将输入的数据向量化完成前向传播和后向传播


> 关于numpy的广播原理

```
#a,b均为矩阵
y = a + 1

y = a + b
```

在对矩阵做加减的过程中，变量会自动扩展为较大矩阵的维度


```
a = np.random.randn(5) #会产生秩的矩阵 而不是行向量或列向量

a = np.random.randn(5, 1) #会产生列向量
```


##  神经网络介绍

### 简单的神经元

![image.png-78.2kB](http://static.zybuluo.com/ShowArcher/dn9yqqfmt9jdp8km7ye67kyd/image.png)
![image.png-70.3kB](http://static.zybuluo.com/ShowArcher/rqr48gio57ly37mndjxpsh3q/image.png)

神经元所做的运算和逻辑回归类似，计算目标函数，在通过激活函数输出。其中[1]表示第一层网络，一个神经元的计算可以归纳为：
$$ Z^{[1]} = W^{[1]}X+b^{[1]}$$
$$ a^{[1]} = \sigma(Z^{[1]}) $$

最终输出$a$

    在计算网络的层数，默认网络的输入层为第0层不计入计数。
### 神经元的向量化处理

![image.png-303.9kB](http://static.zybuluo.com/ShowArcher/vewz8lc1mgd7glwccq8obvgr/image.png)
![image.png-135.6kB](http://static.zybuluo.com/ShowArcher/db8f2kp9f2o6b1g1z2pujlbw/image.png)

通过矩阵的形式将输入权重堆积在一起，加快网络的运算。

### 激活函数

> 为什么使用激活函数

- 引入激活函数是为了将线性的系统改变成非线性的系统 如果没有激活函数 再多的网络层也只是一个线性组合。
- 激活函数有归一化的作用，让输出保持在一个区间，提高系统的稳定性。
- 解决线性不可分的问题



> sigmod

![image.png-65.2kB](http://static.zybuluo.com/ShowArcher/1gqsp2d912hoc8a2b5amkffi/image.png)

$$ \sigma(z) = \dfrac{1}{1+e^{-z}}$$

优点：

   - 适合用二分类问题

缺点：

- 梯度消失发生的概率比较大
- 运算速度慢
    
> tanh

![image.png-115.4kB](http://static.zybuluo.com/ShowArcher/c6pcu2z0sc4rp58oekfuri3a/image.png)

$$tanh(x) = \dfrac{e^x - e^{-x}}{e^x+e^{-x}}$$

tanh(x)只是在sigmod(x)的基础上做了平移，但效果普遍比sigmod好
优点：

- tanh(z) 让数据均值趋于0

缺点：

- 同sigmod一样

> Relu
![image.png-67.6kB](http://static.zybuluo.com/ShowArcher/7embp2dhc2xa3xfti8kdv0zp/image.png)

$$Relu = max(0,x)$$

最常用的激活函数，推荐在人工神经网络的时候优先使用
优点：
    
- 解决了梯度消失的问题
- 计算速度非常快
缺点：

- lr太高可能会导致某些权重不会更新
- 均值不会偏于0

> Leaky ReLU函数

![image.png-75.3kB](http://static.zybuluo.com/ShowArcher/0vn1mjtn98jn926pv3w2zpcx/image.png)

$$PRelu = max(\alpha x,x)$$

PRelu时对Relu的改进，主要针对ReLU的缺点进行改进。实际应用情况不一定会比Relu好



### 权重初始化

如果W权重全部初始化为零，那么所有的隐藏单元都是对称的，即所在的运算都是冗余的。
b可以初始化为0。

```
eg:
W = np.random.randn() * 0.01
b = 0
```
如上所示， W在初始化时，往往会乘0.01。因为如果使用sigmod或tanh激活函数，$y = W^Tx + b$计算出的y值将变大，造成训练初期的偏导太小，训练变慢。


## 深度神经网络


### 前向传播和后向传播

![image.png-130.1kB](http://static.zybuluo.com/ShowArcher/f6976hpihduqrwzkcvtvhamu/image.png)

和逻辑回归类似，前向传播计算值，后向传播计算偏导和梯度.
前向传播过程中会缓存Z,W,B,是为了在反向传播中更快的计算梯度


### 核对矩阵的维数

在调试程序时，最好把网络各个参数的维度在纸上写出来。网络的训练重要的还是数据的构建。

### 为什么需要DNN

![image.png-420.3kB](http://static.zybuluo.com/ShowArcher/qse8djztcrkparov05nyso9j/image.png)

可以看到一个深度神经网络，第一层可能只是学习一些底部的信息，比如检测图片的边缘。第二层则是提取图片的特征，比如眼睛、耳朵。第三层则是对网络特征进行进一步的学习，将各个特征排列组合在一起进行学习，最后得到输出的结果

另外针对同一个问题，根据电路理论，深度神经网络可能只需要O(logn)的时间复杂度， 而单层神经网络可能需要O(2^n)的时间复杂度。

### 参数和超参数

参数：网络的属性值

- 权重w
- 偏差b

超参数：训练过程决定网络训练结果的属性值

- learing_rate 学习率
- epoch 迭代次数
- batch size 图片批的多少
- activation function 激活函数
- loss function 损失函数
- momotan 动量
.....

对于超参数，需要多去实践，才能提取正确的模型参数。


## 网络的训练

### 数据集的划分

一般会将数据集划分为训练集、验证集、测试集。划分比例一般为：6：2：2。
但在数据集很多时，可能会用新的比例：
1w张数据集： 1000张验证集 1000张测试集

### 方差和偏差

偏差高 训练集表现效果低 欠拟合
方差高 验证集表现效果低 过拟合

注意：只有在存在过拟合现象时，才进行过拟合的解决算法

### 解决高方差（过拟合问题） L2正则化

思路：增加数据或者正则化

> L2正则化 在目标函数上添加一项

$$J(w, b) = \dfrac{1}{m}\sum_{i=1}^{m}L(\hat{y}, y) + \dfrac{\lambda}{2m}\sum_{i=1}^mW^2$$

![image.png-89.2kB](http://static.zybuluo.com/ShowArcher/2d51k2tjlc1n4c2seyzowc3r/image.png)
引入后会造成权重衰减的情况，经过激活函数后会导致其线性更强

### dropout(随机失活正则化)

一般放在全连接后、relu后

>思路
    
    在训练过程随机让某些神经元失去作用。
    
>原理
    
    因为dropout对神经元采取随机删除的形式，所以每个神经元不会依赖于输入的特征，所以权重将会均匀的分配到每个输入神经元上
    
>实现

![image.png-85.1kB](http://static.zybuluo.com/ShowArcher/oulnin9aqopytj859iyo3ozm/image.png)

设置一个超参$keep\_prob$，表示对每个神经元有$1-keep\_prob$的概率会删除

### 过拟合

数据增强、early stopping

## 数据归一化

>思路

    通过数据归一化，可以加快网络的训练速度，防止数值溢出。针对的时输入层
    
>实现

![image.png-95.6kB](http://static.zybuluo.com/ShowArcher/qxrwu15o0yt69rinvk5f9zsx/image.png)

将数据归一化为：均值为0，方差为1的分布

>特点
![image.png-191.1kB](http://static.zybuluo.com/ShowArcher/0zh2cmrltzw5gopfek20hbjm/image.png)

没有归一化的数据需要的迭代次数可能更长，归一化后的数据迭代次数会好点

## 梯度

### 梯度消失和梯度爆炸

$$y = W^Lx + b$$

根据如上的公式，其中L代表网络的层数，y和W基本呈指数关系。所以有：

$$W<1 \quad W^L \approx 0$$
$$W>1 \quad W^L \approx 1$$


### 网络权重的初始化

pytorch采用的参数初始化:


![image.png-19.1kB](http://static.zybuluo.com/ShowArcher/bes3cvrjxafwglrsc477wmn9/image.png)


一般采用：

    W = np.random.rand(shape)∗np.sqrt(1/n）
    n表示输入参数的维度，采用高斯分布的形式。
    
如果是RELU激活函数：

    W = np.random.rand(shape)∗np.sqrt(2/n）
    
tanh激活函数（Xavier初始化）：

    W = np.random.rand(shape)∗np.sqrt(1/n）
    
    
### 梯度检验

>思路

    梯度检验是为了检验反向传播过程中的梯度是否计算错误

>实现

- step1 计算双边误差

    $$d\theta_{apporx} = \dfrac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2\varepsilon}$$
    
- step2 计算近似梯度和真实梯度的误差

![image.png-24.8kB](http://static.zybuluo.com/ShowArcher/kb5fc1nrp4v51qaxu7onuf40/image.png)

- step3 验证误差是否小于阈值

>实现细节

- 不要在训练中使用梯度检验，它只用于调试
- 算法的梯度检验失败，要检查所有项，检查每一项，并试着找出bug
- 实施梯度检验时，如果使用正则化，请注意正则项,对正则项也需要求偏导
- 梯度检验不能与dropout同时使用
- 在随机初始化过程中，运行梯度检验，然后再训练网络， w ww 和 b bb 会有一段时间远离0，如果随机初始化值比较小，反复训练网络之后，再重新运行梯度检验


## 优化算法（优化器）

### 梯度算法

|算法|介绍|
|-------|---------------|
|随机梯度下降|每次随机训练一批图片|
|mini-batch|每次训练一定批次图片|
|batch|每次训练所有图片|

#### 原理

![image.png-71.8kB](http://static.zybuluo.com/ShowArcher/u49ssdywm1hzjych4di2xvt4/image.png)

mini-batch想对比batch梯度下降，会产生较多的噪声。

#### mini-batch特点

    mini-batch相较与其他两个算法，向量化加速较好，且训练速度较快。
    一般mini-batch选取批大小为数据规模(m)中间
    

### 指数加权平均


>计算方式

$$v_t = \beta v_{t-1} + (1 - \beta)\theta_t$$

对公式的理解：

- 任意项公式展开后，其系数近似等于1.
- 在编程实现过程中，其运算速度更快，所需内存更低。
- 近似平均了前$\dfrac{1}{1-\beta}$次

###  指数加权平均的偏差修正

![image.png-169.9kB](http://static.zybuluo.com/ShowArcher/qnzy0dd9lztvik8l87vuwtef/image.png)


即在评估的初期不是用$v_t$，而是使用$\dfrac{v_t}{1 - \beta^t}$,这样做可以消除偏差。当t很大时，偏差修正几乎没有作用。

### 动量梯度下降（momentum）

![image.png-147.9kB](http://static.zybuluo.com/ShowArcher/shpeet6jnxi0vaq8507yal2l/image.png)

普通的梯度下降在纵向方向下降较慢，所以引进指数加权平均，加快横向的下降速度，降低纵向的下降速度。
训练过程抽象为一个下坡过程，可以理解w为初速度，dw为加速度。

> 思路

![image.png-47.8kB](http://static.zybuluo.com/ShowArcher/uip42b6we4acbc3k4ku589qq/image.png)

注意：动量梯度下降一般情况较好，但在碗形结构下降表现较差。

### 均方根(RMSprop)

![image.png-134.8kB](http://static.zybuluo.com/ShowArcher/el25f4uih8l3ndneptf94px7/image.png)

>提出

为了解决动量梯度下降会造成纵向下降减慢的问题

>算法思路

对dw和db进行平方操作，这样可以使Sw变得更小，Sdb变得更大。
进行w和b的更新后，w就会变大，b就会变小。
为了稳定可以在更新w和b的时候加上一个常量

### adam算法


![image.png-116kB](http://static.zybuluo.com/ShowArcher/3fqxe8fjl4p05axzg0xtj1l5/image.png)

adam结合动量梯度下降和RMS算法，使之能在各种情况下都适应的算法

### 学习率下降

> decay-rate

$$\alpha = \dfrac{1}{1 + decayrate*epoch} \times \alpha_0$$


>其他的方式


![image.png-27.8kB](http://static.zybuluo.com/ShowArcher/q1raa7h5pt6ojbuhejkgq4y9/image.png)

指数下降、批次下降

## 参数调整

### 超参

重要性依次下降

- learing_rate 学习率
- $\beta$ 动量梯度下降的参数
- mini-batch_size 图片批的多少
- learning rate decay 学习率下降
- epoch 迭代次数
- activation function 激活函数
- loss function 损失函数


### 超参选择

![image.png-108.5kB](http://static.zybuluo.com/ShowArcher/g69dxrh8adpts3cjz57ocejq/image.png)

一般不会采取均匀采样，而是平均采样。因为参数的重要性并不一致。

>调参的两种方式

Pandas:只训练一个模型，每隔几次观察模型效果再进行调参
Caviar:训练多个模型，比较哪个参数最好


### BN层

#### 思路

    将归一化的思想运用在隐藏层上，归一化均值和方差
    
#### 实现

![image.png-108.6kB](http://static.zybuluo.com/ShowArcher/fc6n0dtn2x1csawiqoplr657/image.png)

为了使归一化后的网络具有不同的分布，引进了参数$\zeta$和$、beta$.
注意：一般是在激活函数前使用BN.

#### 特点

因为归一化会减去平均值，所以参数$b$失去了作用，可以直接设置为0

#### 思想

![image.png-120.9kB](http://static.zybuluo.com/ShowArcher/zk2q3ph1ohxdzf7u1sb1k25z/image.png)

限制了在前层的参数更新，会影响数值分布的程度，第三层看到的这种情况，因此得到学习。Batch归一化减少了输入值改变的问题，它的确使这些值变得更稳定，神经网络的之后层就会有更坚实的基础。即使使输入分布改变了一些，它会改变得更少。它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，你可以这样想，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。


引进BN层的原因：

- 使后层网络适应前层神经元的输入，能使数据更加稳定
- 可以降低数据过拟合

#### 测试时的BN

![image.png-121.4kB](http://static.zybuluo.com/ShowArcher/49qfjadvtfefgjgses7niczf/image.png)

训练时，是通过一个mini-batch来进行归一化的。但在测试时，只有一个数据无法进行归一化，所以对训练的均值和方差进行指数加权平均作为测试的参数。

### softmax层

![image.png-169.3kB](http://static.zybuluo.com/ShowArcher/l27p58kn8w9lvleqnhupcdot/image.png)

对最好的输出做如下处理：取幂指数，再归一化。这样输出的值代表每个类的概率。
功能：
输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。

注意：不要和交叉损失函数一起使用，因为pytorch中的交叉损失函数包括了softemax

## 深度学习优化策略

在实际开发过程，比如模型训练集表现准确率为90%，而在模型测试集上表现效果较差。
你可以选择调节优化算法、学习率、dropout、l2正则化。但如何选择合适的策略取抉择。

### 决策正交化

>举个例子：一个按钮可以调节速度和方向，我们将其正交化为两个独立的决策速度和方向进行调整

![image.png-111.3kB](http://static.zybuluo.com/ShowArcher/4iqxr0omnad2glqktcduoifu/image.png)

如上图所示：改变网络结构会同时影响训练集、验证集、测试集的效果。这时选择较为独立的正交决策：增加数据集、正则化来加强验证集的效果

### 指标单一化

有多个指标时考虑将指标通过组合选择一个最好的，这会加速你的网络迭代速度。

### 数据集的决策

在实际开发过程中，尽量让验证集(开发集)和测试集来自于同一个分布。

### 定义一个新的指标（损失函数）

加入在实际开发猫狗识别项目中，模型虽然表现很好，但会错误的将很多色情图片分类为猫，这时就可以改变评价指标。
原来的指标如下,表示错误样本的比率：medv代表开发集

$$Error = \frac{1}{m_{dev}}\sum_{i=1}^{m_dev}I(y_{pred}not = y)$$

先在添加一个权重，误测为色情图片时，权重设置为10，其他为1，并将其归一化到(0,1),如下：d

$$Error = \frac{1}{\sum{w^i}}\sum_{i=1}^{m_dev}w^iI(y_{pred}not = y)$$


### 贝叶斯错误率

考虑人类水平错误率的界限时，可以用贝叶斯错误率作为替代或估计

### 端到端的学习

假设有一个任务：需要分几步来处理，特征选择、特征提取等等。
而端到端则是直接省略点中间的步骤过程，直接一步到位。前提需要大量的数据。

> 数据量越大 越可以减少中间的步骤
实际过程中，除非数量量级达到很多程度才使用端到端的学习

## 卷积神经网络

卷积神经网络一般由卷积层、池化层、全连接层组成。


### 卷积操作基础

![image.png-242.3kB](http://static.zybuluo.com/ShowArcher/yuoy7iaw1d36jdp2b38rpwi1/image.png)

参数介绍:

- 通道数即RGB对应的通道
- 卷积核和过滤器即进行卷积的参数
- 卷积核的通道数和输入图像的通道数相同
- 将卷积核对应的权重与输入图像对应的参数相乘再求和得到输出

多个卷积核的卷积操作如下：

![image.png-613.3kB](http://static.zybuluo.com/ShowArcher/1s4pnyuafdct9tf7eb3t55bs/image.png)


如上图所示，将对应的卷积核和输入图像相乘加上一个偏差，再将所有的结果堆叠。
所以卷积核的个数对应输出图像的通道数。



### 卷积层

#### 输入和输出的相关参数计算

![image.png-490kB](http://static.zybuluo.com/ShowArcher/8lt85zyr8kphj5i1b48yvjpf/image.png)

$$N_w = \frac{N_w^{L-1}+2P-f}{S} + 1 向下取整$$


### 池化层

池化与卷积的操作类似，非为最大池化、平均池化、最小池化。一般为最大池化。
作用主要是为了提升网络的效率。

注意：
池化没有参数需要学习。实际过程中只有一个卷积核（且为单通道），分别对每个通道的图像进行池化，再堆叠。所以池化后的通道数一定原来的一样。

### 总结

因为池化层没有参数需要学习，所以一般会将卷积层和池化作为一层网络（layer）

## 参数共享和稀疏连接

### 参数共享

即对某种功能或者某种特征提取其卷积核的参数应大致一致。
所以可以针对不同的任务，可以利用预训练的参数进行迁移学习。

### 稀疏连接

即卷积层输出的一个指只利用了上一层网络的部分数据，而全连接的一个值利用了上一层的所有数据。

## 经典神经网络

### LeNet-5

![image.png-391.3kB](http://static.zybuluo.com/ShowArcher/17arryoaeqi2ibqt2qkm90xm/image.png)

Lenet-5针对的是灰度图像的字符识别。总体结构采用卷积、池化、激活函数的形式，网络的输出趋势为长宽降低、通道加深。

### ALexNet

![image.png-542.3kB](http://static.zybuluo.com/ShowArcher/bmulot8sbdld36j6u1gf4tbt/image.png)

ALexNet,与LeNet-5类似，但其网络结构会更大。另外引进了Relu激活函数使网络训练速度更快。
提出了多gpu训练的方式，即让网络层分别位于不同的GPU上，并行运行。

### VGG-16

![image.png-484.3kB](http://static.zybuluo.com/ShowArcher/efd7syghad86e89ehrzmaoz9/image.png)

VGG-16，指用了16层卷积和池化层。其网络输出的趋势保持长宽减半，通道增倍的方式。网络结构采取几个卷积后加入池化的方式。网络参数138m，比较复杂。

## 残差网络

![image.png-420kB](http://static.zybuluo.com/ShowArcher/947naqsfnhrz1y6vhk3sefvk/image.png)


残差网络由许多的残差块组成。残差块是将前面的输出特征和当前的输出特征合并相加。
注意是加在了激活函数前面。

残差网络的作用：

- 快速解决恒等变换问题
- 解决梯度消失和梯度爆炸问题
- 网络层教深的情况下，能避免网络退化，即网络随着层数加深，效果不会变差

## 1*1的卷积网络

1*1的卷积网络类似于全连接，它将一副图片所有的通道单元进行全连接合并成一个。
所以1*1的卷积网络可以改变图像的通道大小，池化可以改变长宽。

功能：

- 改变图像的通道数量
- 引进非线性特性
- 并且参数量小，速度快。

## Inception网络

### 基础网络块

![2022-01-21 15-30-39 的屏幕截图.png-170.5kB](http://static.zybuluo.com/ShowArcher/omsy6mnehct12200tree6zov/2022-01-21%2015-30-39%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png)

Inception网络由如上基础网络块组成，其设计思想：让网络训练过程自己去决定使用哪个网络层。包括卷积和池化。

加速过程：

![2022-01-21 15-36-58 的屏幕截图.png-172.1kB](http://static.zybuluo.com/ShowArcher/hrplp5unr5p50c1z6lw1r70b/2022-01-21%2015-36-58%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png)

如果直接采取端到端的方式，其计算量能达到1200M。
采取1*1的卷积层进行改进，也称为瓶颈层。先将网络的通道数进行压缩，再进行卷积，计算量可以减至120M

### 总体结构

![2022-01-21 15-42-26 的屏幕截图.png-253.8kB](http://static.zybuluo.com/ShowArcher/u8oitd6hs9ztivgrpuigbk2q/2022-01-21%2015-42-26%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png)


利用基础的Inception块不断堆叠，再外加全连接、sofemax进行输出。
中间会对隐藏层加入全连接，再将所有的全连接输出进行sofemax输出结果。这样可以减少过拟合的问题。

>彩蛋：Inception网络来自盗梦空间中一句话：We Need to go deeper.

## 迁移学习

![2022-01-21 16-02-52 的屏幕截图.png-306kB](http://static.zybuluo.com/ShowArcher/wewwxbx05u1ga1d9hl24rvy6/2022-01-21%2016-02-52%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png)

>freeze: 表示冻结网络层，不用训练。

需要训练一个猫狗识别的网络，可以直接将别人的网络和权重利用过来，然后修改后面的全连接和softmax层即可。
但随着数据集的增加和任务的复杂，冻结的网络层会变少。

## 数据增强

图像变换：

- 镜像对称
- 随机裁剪
- 图像旋转扭曲

色彩装换：

- RGB通道上加入失真
- PCA颜色增强

## 科研任务和工程任务

![image.png-466.2kB](http://static.zybuluo.com/ShowArcher/44lldj4cnff1ipvom1eboib1/image.png)

科研任务采用的方法：

- 集成学习:同时训练多个网络，然和平均它们的输出结果
- Multi-crop：对测试集进行扩展，取其输出的平均。比如：如上图所示分别取各中心和四个角的区域进行测试。

如上都可提升模型的精度

工程任务：

- 使用开源的网络和代码结构
- 最好有一个好的与训练模型然后微调。

# 目标检测

## 分类定位

分类定位针对图像中只含有一个对象的情况，一般采取直接在网络的softmax加入坐标的输出即可

![image.png-466.2kB](http://static.zybuluo.com/ShowArcher/co1x17gb1u8axpu6f7v6x4ew/image.png)

![image.png-571.8kB](http://static.zybuluo.com/ShowArcher/a1d75wmpef85ew9ayrj0mnvf/image.png)

在对此类问题进行训练的时候，一般采取对Pc使用逻辑回归函数，坐标采用平方差之和的函数来进行训练
如果是背景图片，则损失函数应只关于P

> 关键点的检测问题也是采取同样的方法

## 目标检测

### 滑动窗口法

目标检测即含有多个对象需要分类定位, 此时采用的方法是滑动窗口法，即一个固定大小的窗口滑动输入到分类设备网络中进行预测。

![image.png-575.1kB](http://static.zybuluo.com/ShowArcher/zynm34wsmqdq6ntmdpecmvee/image.png)

因为一个一个的滑动窗口所需要的算力较大，所以采用将卷积的形式替代全连接，使网络可以同时处理所有的窗口。

具体做法：

- 设计一个14*14*3的分类识别网络
- 保持网络的架构应用到16*16*3的图片上，最后输出的2*2*4即对于滑动的四个窗口
- 注意因为池化是2*2,所以第一步的卷积步长为2.

但滑动窗口法的预测框有所偏差。

### YOLO

![2022-01-25 11-19-35 的屏幕截图.png-410kB](http://static.zybuluo.com/ShowArcher/73gjs0t654qf4syk8njtrd5l/2022-01-25%2011-19-35%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png)

YOLO将输入图像划分为独立网格，然后将每个网格输入到网络中进行预测。

具体做法：

- 将网络分成独立3*3网格，通过卷积的方法实现网格划分，最后输出3*3*8
- 如果检测对象中心在网格中，Pc为1
- 目标框的大小可以超过网格大小

注意：这样可以让预测框不再受限滑动窗口法的步长，每个框内只检测一个对象。

### NMS

![2022-01-25 12-31-01 的屏幕截图.png-491.9kB](http://static.zybuluo.com/ShowArcher/9fk79lc795lh99zxij0aon99/2022-01-25%2012-31-01%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png)

Pc代表boundingbox的置信度，表示框里含有检测对象的概率。因为实际检测同一个目标可能有多个网格都认为中心在其中，所以有多个bbox，这时需要利用NMS去除多余的框
预测时的步骤：

- 去除Pc较小的bbox
- 使用nms抑制其他框。

### Anchor box

Anchor box 也叫做先验框，是为了处理同一个网格中又两种对象的情况。
此时每一个对象都会分配到一个网格和一个anchor box中。所以此时的输出会从3*3*8变成3*3*（8*2）。

### YOLO

![image.png-712.5kB](http://static.zybuluo.com/ShowArcher/sjgyvfjtjmjccmng46530ror/image.png)

- 输入一副100*100*3的图片输出3*3*（8*2）的结果。
- 去除每个网格中置信度较低的预测框
- 对每个类别运行一次NMS算法

### 候选区域和RCNN系列

![image.png-792.6kB](http://static.zybuluo.com/ShowArcher/m53s7bt3vwo3z1bd1b89yr89/image.png)

RCNN系列采用两步走的方法，即先使用分割算法找出感兴趣的区域块，再在各区域块中运行检测算法。
faster-rcnn改进了传统分割算法，采用卷积来实现分割的过程。

## 卷积神经网路的应用

### 人脸识别

人脸验证： 输入图像和名字，验证两者是否一致。
人脸识别： 输入图像，识别出名字。

### one-shot learning(一次学习)

one-shot learning(一次学习): 指训练集中每个样本只有一个数据，来完成人脸识别。
一般的做法是采用一个差异值来解决，即输入图片和数据库中的图片做对比，当差异值小于一个阈值时，将其作为识别的结果。

### siamese network

![image.png-181.2kB](http://static.zybuluo.com/ShowArcher/xehnp2ltujlghupdt0v2rw5s/image.png)

Siamese network人脸识别网络，依然是采取CNN的形式，只不过最后输出的是全连接后的一个向量。
然后对两个图片的网络输出求出二范数做为差异值。

### siamese的训练

![image.png-316.4kB](http://static.zybuluo.com/ShowArcher/042jxfvqa1hztdp58q4c4s6k/image.png)

网络的训练需要三组数据：Anchor(原始数据)、Positive（正样本）、Negative（负样本）。
梯度下降中的损失函数为：

$$loss = MAX(|f(A)-f(P)|^2 - |f(A)-f(N)|^2 + \alpha , 0)$$

这里加入$\alpha$超参是为了避免网络只输出0的情况，采用max后网络对小于0的loss将统一视为0.

### 用二分类实现人脸识别

![image.png-652.1kB](http://static.zybuluo.com/ShowArcher/1j16gk7jh6ftdht27kul84g8/image.png)

同样利用对比的思想，只不过采取在输出向量后接一个softmax输出结果y（是或否）。
加快部署速度：在数据集库中存储的是图片将网络后的向量。需要识别时直接进行对比。

### 神经风格转移

![image.png-510kB](http://static.zybuluo.com/ShowArcher/0l08140jy5gyge1tf0awdju3/image.png)

神经风格转移，即将一个图像的风格进行转换为另一个图像的风格。

### 深度神经网络

![image.png-670.5kB](http://static.zybuluo.com/ShowArcher/akltfh8dm2py47od15q7n6zp/image.png)

如上图所示，大块代表9个激化单元，每个激活单元里面有9个图片。神经网络在每一层都在输出不同的特征。

### 神经网络转移的代价函数

![image.png-17.9kB](http://static.zybuluo.com/ShowArcher/aw214a99gp1bodvf4g9ndpdq/image.png)

其代价函数由两部分组成：内容代价函数和风格代价函数（G代表生成的图像）

>内容代价函数

![image.png-256.3kB](http://static.zybuluo.com/ShowArcher/h0g0grqab0ox4qeyj8t8qnnt/image.png)

内容代价函数可以由二范式来组成，注意计算的代价函数是经网络输出的隐藏单元。

>风格代价函数

![image.png-188.7kB](http://static.zybuluo.com/ShowArcher/tzj2e4maphmh2li354xvrkkr/image.png)

风格代价函数的计算主要通过计算不同激活单元的相关系数来表示。不同激化单元的相关系数可以理解为图像即出现竖直特征又出现橙色的系数。

### 3d卷积

![image.png-177.1kB](http://static.zybuluo.com/ShowArcher/dveusc1sbj6kulu7i38b3jau/image.png)

与2d卷积类似，卷积核的大小由二维变成了三维，卷积核的个数觉得最终输出的通道数。

# 深度学习实践工程

## warm up

Warm up是一种训练思想，在模型预训练阶段，先使用较小的学习率训练一些epochs或者steps(比如4个epoches或10000steps)，
再修改为预先设置的学习率进行训练。

例如: 在ResNet网络中先使用0.01学习率训练模型至误差低于80%(大概训练400个steps)，然后使用0.1学习率进行训练。

## mixup

Mixup是一种数据增广策略，通过对模型输入与标签构建具有“凸”性质的运算，构造新的训练样本与对应的标签，提高模型的泛化能力。
并且能够提高模型对于对抗攻击(Adversarial Attack)的鲁棒性。

![image.png-30.6kB](http://static.zybuluo.com/ShowArcher/eb35adxxmd8c9qqq9v5hiid0/image.png)

其中xi,yi和xj,yj都是从训练集中随机选择的，其中lambda取值于beta分布，范围为0-1。

>在大部分半监督场景下，当[公式]的取值在[公式]附近时，如[公式]，模型泛化能力会提高一个台阶（如在DomainNet上相对于[公式]的分布会提高4-5个百分点）。对于这种现象，现有所有的理论似乎都失效了，这也是值得研究的地方。


## mainifold mixup（流行混合算法）

manifold mixup是对mixup的扩展，把输入数据（raw input data）混合扩展到对中间隐层输出混合。
就是将mixup操作泛化到特征上；特征具有更高阶的语义信息，在其维度上插值可能会产生更有意义的样本。

求步骤如下：

- 随机选网络的某层第k层（包括输入）；
- 传两批数据给网络，前向传播到第k层，得到隐藏标准gk；
- 使用mixup
![2022-03-02 20-43-51 的屏幕截图.png-11.6kB](http://static.zybuluo.com/ShowArcher/gcc6q8088leu7s7d8k4o1gfi/2022-03-02%2020-43-51%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png)
- 继续前向传播得到输出；
- 计算损失和梯度



  [1]: 


  [1]:




  [1]: 


  [1]: 